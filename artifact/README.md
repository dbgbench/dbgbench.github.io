
## How to use the ESEC/FSE 2017 artifact

The `DBGBENCH` artifact provides materials drawn from a large-scale debugging study with human participants. 
A comprehensive description of the shared dataset and infrastructure is provided in the 
<a href="https://dbgbench.github.io">project website</a> In general, the provided artifact can be used for 
several purposes that might well go beyond the scope of our ESEC/FSE 2017 paper. In the following, we will 
explicitly outline two usage scenarios to help understand the scope of our work. 


### Extending the DBGBENCH dataset

In order to carry out a similar study and extend the `DBGBENCH` dataset, we provide all questionnaire and 
the necessary computational infrastructure. Specifically, the following steps should be carried out in 
order to replicate the study environment:

1. [Install the docker virtual infrastructure](../docker). This docker 
includes all the necessary source code as well as tests to manifest the investigated bugs. It can be shared 
with the study participant. 

2. <a href="https://drive.google.com/open?id=0Bx6dkN27OssKVWJYZGdXcWdWQ0U">Download tutorial materials</a> 
that include slides and videos to provide details about the study to participants. 

3. [Download example questionnaire](../questionnaire.pdf) for the 
study participants. For each bug under investigation, this questionnaire needs to filled up by a participant 
after she finishes debugging. The questionnaire can be set up in an online form (e.g. Google form) to 
make it easily accessible by the participants. 


### Using the DBGBENCH dataset

The `DBGBENCH` dataset collected data from 12 professional developers debugging 27 real-world bugs. 
This dataset is provided in a comprehensive format in the <a href="https://dbgbench.github.io">project website</a>.

One of the primary usage of `DBGBENCH` dataset is to compare the effectiveness of automated debugging tools, 
in particular, **automated fault localization** and **automated repair** tools. In the following, we 
outline a few examples:

1. We provide scripts to check the plausibility of patches provided by participants. Please follow these 
[instructions](../patches) to apply and check plausibility of 
participant-provided patches. 

2. We include fault locations as provided by the participants. These fault locations are manually cross 
checked to validate their correctness. See the [plaintext version](../find.24e2271e.faults.txt) 
of such a fault location for the bug `find.24e2271e`.These fault locations can be used for validating an automated 
fault localization tool. As an example, for statistical fault localization tools, participant-provided fault locations 
can be compared with the most suspicious statements reported by the fault localization tool. 

3. We provide participant-provided patches for each bug. For example, see [all the patches and their categorizations](../patches/find.24e2271e) (plausible and/or correct) provided for the bug `find.24e2271e`. 
These patches can be used to compare the readability, structure and correctness of patches generated by 
automated repair tools.

4. We include bug diagnosis results (in natural language) provided by the study participants. See the 
[plaintext version](../find.24e2271e.diagnosis.txt) of such diagnosis 
for the bug `find.24e2271e`. Such diagnoses can be leveraged upon to generate natural-language explanations 
of common bug types. Such explanations can further be used to design and evaluate sophisticated debugging 
tools that highlight suspicious locations along with a possible explanation of the bug. 

5. We provide data on the average time and the average number of correct fixes for each bug in the 
<a href="https://dbgbench.github.io">project website</a>. This data can be used to evaluate automated 
debugging tools in the future. In particular, we hypothesize that a necessary (but not sufficient) condition to 
validate automated fault localization and repair tools is to outperform the study participants. 


### Final note

* The entire **raw data** of `DBGBENCH` can be downloaded as a [self-containted CSV file](../dbgbench.raw.csv).
* The entire **cleaned data** of `DBGBENCH` is available as [repository](https://github.com/dbgbench/dbgbench.github.io). Pull requests welcome!
* A **summary** of the data can be downloaded as [PDF file](../dbgbench.summary.pdf)
* Apart from the aforementioned use cases, researchers and professionals may take advantage of this dataset 
in ways that best match their research interests. 






